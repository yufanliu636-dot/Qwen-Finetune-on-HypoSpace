# merge_weight_final.py
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

print("ğŸ”¹ å¼€å§‹åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹ä¸­...")

# è®¾ç½®è·¯å¾„ - ä½¿ç”¨æœ€ç»ˆçš„æ£€æŸ¥ç‚¹
base_model_path = r"/opt/data/private/Qwen2.5-14B"
adapter_path = r"/opt/data/private/causal/checkpoint-200"  # ä½¿ç”¨æœ€ç»ˆæ£€æŸ¥ç‚¹
merged_model_path = r"/opt/data/private/causal/checkpoint-200"

try:
    # æ£€æŸ¥æ£€æŸ¥ç‚¹ç›®å½•
    print(f"ğŸ”¹ ä½¿ç”¨æ£€æŸ¥ç‚¹: {adapter_path}")
    if not os.path.exists(adapter_path):
        print(f"âŒ æ£€æŸ¥ç‚¹è·¯å¾„ä¸å­˜åœ¨: {adapter_path}")
        exit(1)
    
    config_file = os.path.join(adapter_path, "adapter_config.json")
    if not os.path.exists(config_file):
        print(f"âŒ åœ¨æ£€æŸ¥ç‚¹ä¸­æ‰¾ä¸åˆ° adapter_config.json")
        exit(1)
    
    print("ğŸ”¹ åŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    
    print("ğŸ”¹ åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    print("ğŸ”¹ ä»æ£€æŸ¥ç‚¹åŠ è½½é€‚é…å™¨å¹¶åˆå¹¶æƒé‡...")
    model = PeftModel.from_pretrained(base_model, adapter_path)
    merged_model = model.merge_and_unload()
    
    print(f"ğŸ”¹ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {merged_model_path}")
    merged_model.save_pretrained(merged_model_path)
    tokenizer.save_pretrained(merged_model_path)
    
    print("âœ… æƒé‡åˆå¹¶å®Œæˆï¼")
    print(f"   åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"   é€‚é…å™¨: {adapter_path}") 
    print(f"   åˆå¹¶åæ¨¡å‹: {merged_model_path}")
    
except Exception as e:
    print(f"âŒ æƒé‡åˆå¹¶å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()